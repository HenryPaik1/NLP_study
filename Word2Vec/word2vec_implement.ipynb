{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code: https://github.com/lazyprogrammer/machine_learning_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "sentences = [[13, 100, 100, 100, 100, 74, 100, 47, 100, 16, 100, 100, 100, 100, 100, 26, 71, 100, 27, 21, 97, 100, 100, 100, 15], [13, 100, 100, 74, 20, 100, 100, 21, 13, 100, 100, 100, 14, 49, 39, 100, 100, 16, 13, 100, 14, 26, 100, 13, 100, 17, 100, 16, 13, 100, 16, 100, 27, 25, 13, 100, 20, 49, 13, 100, 23, 100, 15], [13, 100, 100, 100, 39, 62, 100, 36, 100, 100, 100, 100, 100, 100, 18, 100, 100, 16, 100, 26, 100, 27, 20, 13, 100, 100, 49, 23, 100, 36, 100, 100, 100, 100, 15], [26, 84, 19, 100, 100, 16, 100, 100, 23, 100, 27, 14, 13, 100, 74, 14, 26, 100, 13, 100, 100, 20, 13, 100, 14, 13, 100, 16, 100, 17, 13, 100, 16, 38, 100, 27, 15], [13, 100, 74, 28, 100, 100, 21, 100, 16, 100, 100, 17, 100, 100, 26, 42, 100, 45, 100, 17, 100, 100, 27, 15], [28, 100, 21, 100, 100, 100, 26, 18, 46, 91, 100, 100, 17, 100, 18, 13, 100, 16, 100, 17, 100, 82, 27, 15], [13, 100, 100, 100, 32, 19, 100, 16, 85, 100, 14, 100, 82, 13, 100, 17, 100, 100, 100, 100, 49, 28, 74, 26, 42, 100, 100, 17, 100, 100, 100, 100, 49, 100, 18, 13, 100, 100, 16, 100, 100, 27, 15], [100, 100], [100, 14, 13, 100, 74, 28, 100, 26, 91, 92, 100, 100, 33, 100, 18, 100, 100, 100, 17, 100, 13, 100, 16, 100, 27, 15], [13, 100, 100, 100, 14, 13, 100, 74, 14, 26, 22, 100, 20, 100, 100, 100, 30, 19, 100, 16, 100, 100, 100, 27, 15]]\n",
    "word2idx={'START': 0, 'END': 1, 'man': 2, 'paris': 3, 'britain': 4, 'england': 5, 'king': 6, 'woman': 7, 'rome': 8, 'london': 9, 'queen': 10, 'italy': 11, 'france': 12, 'the': 13, ',': 14, '.': 15, 'of': 16, 'and': 17, 'to': 18, 'a': 19, 'in': 20, 'that': 21, 'is': 22, 'was': 23, 'he': 24, 'for': 25, '``': 26, \"''\": 27, 'it': 28, 'with': 29, 'as': 30, 'his': 31, 'on': 32, 'be': 33, ';': 34, 'at': 35, 'by': 36, 'i': 37, 'this': 38, 'had': 39, '?': 40, 'not': 41, 'are': 42, 'but': 43, 'from': 44, 'or': 45, 'have': 46, 'an': 47, 'they': 48, 'which': 49, '--': 50, 'one': 51, 'you': 52, 'were': 53, 'her': 54, 'all': 55, 'she': 56, 'there': 57, 'would': 58, 'their': 59, 'we': 60, 'him': 61, 'been': 62, ')': 63, 'has': 64, '(': 65, 'when': 66, 'who': 67, 'will': 68, 'more': 69, 'if': 70, 'no': 71, 'out': 72, 'so': 73, 'said': 74, 'what': 75, 'up': 76, 'its': 77, 'about': 78, ':': 79, 'into': 80, 'than': 81, 'them': 82, 'can': 83, 'only': 84, 'other': 85, 'new': 86, 'some': 87, 'could': 88, 'time': 89, '!': 90, 'these': 91, 'two': 92, 'may': 93, 'then': 94, 'do': 95, 'first': 96, 'any': 97, 'my': 98, 'now': 99, 'UNKNOWN': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START inf\n",
      "END inf\n",
      "man inf\n",
      "paris inf\n",
      "britain inf\n",
      "england inf\n",
      "king inf\n",
      "woman inf\n",
      "rome inf\n",
      "london inf\n",
      "queen inf\n",
      "italy inf\n",
      "france inf\n",
      "the 69971\n",
      ", 58334\n",
      ". 49346\n",
      "of 36412\n",
      "and 28853\n",
      "to 26158\n",
      "a 23195\n",
      "in 21337\n",
      "that 10594\n",
      "is 10109\n",
      "was 9815\n",
      "he 9548\n",
      "for 9489\n",
      "`` 8837\n",
      "'' 8789\n",
      "it 8760\n",
      "with 7289\n",
      "as 7253\n",
      "his 6996\n",
      "on 6741\n",
      "be 6377\n",
      "; 5566\n",
      "at 5372\n",
      "by 5306\n",
      "i 5164\n",
      "this 5145\n",
      "had 5133\n",
      "? 4693\n",
      "not 4610\n",
      "are 4394\n",
      "but 4381\n",
      "from 4370\n",
      "or 4206\n",
      "have 3942\n",
      "an 3740\n",
      "they 3620\n",
      "which 3561\n",
      "-- 3432\n",
      "one 3292\n",
      "you 3286\n",
      "were 3284\n",
      "her 3036\n",
      "all 3001\n",
      "she 2860\n",
      "there 2728\n",
      "would 2714\n",
      "their 2669\n",
      "we 2652\n",
      "him 2619\n",
      "been 2472\n",
      ") 2466\n",
      "has 2437\n",
      "( 2435\n",
      "when 2331\n",
      "who 2252\n",
      "will 2245\n",
      "more 2215\n",
      "if 2198\n",
      "no 2139\n",
      "out 2097\n",
      "so 1985\n",
      "said 1961\n",
      "what 1908\n",
      "up 1890\n",
      "its 1858\n",
      "about 1815\n",
      ": 1795\n",
      "into 1791\n",
      "than 1790\n",
      "them 1788\n",
      "can 1772\n",
      "only 1748\n",
      "other 1702\n",
      "new 1635\n",
      "some 1618\n",
      "could 1601\n",
      "time 1598\n",
      "! 1596\n",
      "these 1573\n",
      "two 1412\n",
      "may 1402\n",
      "then 1380\n",
      "do 1363\n",
      "first 1361\n",
      "any 1344\n",
      "my 1318\n",
      "now 1314\n",
      "Vocab size: 101\n"
     ]
    }
   ],
   "source": [
    "V = len(word2idx)\n",
    "print(\"Vocab size:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "window_size = 2\n",
    "learning_rate = 0.025\n",
    "final_learning_rate = 0.0001\n",
    "num_negatives = 5 # number of negative samples to draw per input word\n",
    "epochs = 20\n",
    "D = 50 # word embedding size\n",
    "learning_rate_delta = (learning_rate - final_learning_rate) / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences, vocab_size):\n",
    "    word_freq = np.ones(vocab_size) * 1e-5\n",
    "    word_count = sum(len(sentence) for sentence in sentences)\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_freq[word] += 1\n",
    "            \n",
    "    # smoothing: 너무 높은 freq, 너무 낮은 freq의 갭을 줄여줌\n",
    "    # eg. 0.71 ** (3/4) = 0.77 vs 0.07 ** (3/4) = 0.14\n",
    "    p_neg = word_freq**0.75 \n",
    "\n",
    "    # normalize it\n",
    "    p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "    assert(np.all(p_neg > 0))\n",
    "    return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "W = np.random.randn(vocab_size, D) # input-to-hidden\n",
    "W2 = np.random.randn(D, vocab_size) # hidden-to-output\n",
    "# sampling 될 확률\n",
    "p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
    "p_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "p_drop = 1 - np.sqrt(threshold / p_neg) # p_neg가 thr보다 크면: \n",
    "                                        # 1-작은값 = p_drop커짐\n",
    "                                        # 즉, p_neg에 비례하여 p_drop커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, sen, window_size):\n",
    "    start = max(0, pos-window_size)\n",
    "    end_ = min(len(sen), pos + window_size)\n",
    "    context = []\n",
    "    for ctx_pos, ctx_word_idx in enumerate(sen[start:end_], start=start):\n",
    "        if ctx_pos == pos:\n",
    "            continue\n",
    "        context.append(ctx_word_idx)\n",
    "    return context\n",
    "\n",
    "def sgd(input_, targets, label, learning_rate, W, W2):\n",
    "    # W[input_] shape: D <- word의 one hot encoding으로 \n",
    "                            # lookup=dense vector \n",
    "    # W2[:,targets] shape: D x N \n",
    "    # activation shape: N\n",
    "    print(\"input_:\", input_, \"targets:\", targets)\n",
    "    activation = W[input_].dot(W2[:, targets])\n",
    "    prob = sigmoid(activation) # softmax 아니지?\n",
    "    \n",
    "    # return cost (binary cross entropy)\n",
    "    cost = label * np.log(prob + 1e-10) + (1 - label) * \\\n",
    "    np.log(1 - prob + 1e-10)\n",
    "\n",
    "    # gradients\n",
    "    gW2 = np.outer(W[input_], prob - label)  # D x N\n",
    "    gW = np.sum((prob - label)*W2[:, targets], axis=1)  # D\n",
    "    \n",
    "    # update weight\n",
    "    W2[:, targets] -= learning_rate*gW2  # D x N\n",
    "    W[input_] -= learning_rate*gW  # D\n",
    "    return cost.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence []\n",
      "\n",
      "sentence []\n",
      "\n",
      "sentence [27]\n",
      "\n",
      "sentence [14]\n",
      "\n",
      "sentence []\n",
      "\n",
      "sentence []\n",
      "\n",
      "sentence []\n",
      "\n",
      "sentence []\n",
      "\n",
      "sentence [28, 27]\n",
      "\n",
      "input 28\n",
      "label data [27]\n",
      "negative sample 13\n",
      "input_: 28 targets: [27]\n",
      "input_: 13 targets: [27]\n",
      "\n",
      "input 27\n",
      "label data [28]\n",
      "negative sample 14\n",
      "input_: 27 targets: [28]\n",
      "input_: 14 targets: [28]\n",
      "\n",
      "sentence []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost = 0\n",
    "for sen in sentences:\n",
    "    # 1 - p_drop = drop되지 않을 확률, 즉 p_drop 크면 w는 살아남지 못함\n",
    "    # negative sample 갯수도 랜덤하게 추출\n",
    "    sen = [w for w in sen if np.random.random() < (1-p_drop[w])]\n",
    "    print('sentence', sen)\n",
    "    print()\n",
    "    if len(sen) < 2:\n",
    "        continue\n",
    "    # center words를 랜덤하게 선택 -> neg\n",
    "    shuffle_words = np.random.choice(len(sen), size=len(sen), replace=False)\n",
    "    for pos in shuffle_words:\n",
    "        word = sen[pos]\n",
    "        print('input', word)\n",
    "        context_words = get_context(pos, sen, window_size)\n",
    "        print('label data', context_words)\n",
    "        neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "        print('negative sample', neg_word)\n",
    "        targets = np.array(context_words)\n",
    "        \n",
    "        # update W: input_=word, target=target,\n",
    "        c = sgd(input_=word, targets=targets, label=1, learning_rate=learning_rate, W=W, W2=W2)\n",
    "        cost += c\n",
    "        c = sgd(neg_word, targets, 0, learning_rate, W, W2)\n",
    "        cost += c\n",
    "        print()\n",
    "# decay learning rate\n",
    "learning_rate -= learning_rate_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 101개 단어에 대한 임베딩 결과물\n",
    "W.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
