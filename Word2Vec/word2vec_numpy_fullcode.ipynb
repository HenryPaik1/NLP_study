{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7) (6, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "# target word는 총 6개(마지막 단어 제외), vocal dict size은 7 dim \n",
    "print(target.shape, contexts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "x = target\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. first epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "t = contexts\n",
    "idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "# idx\n",
    "# >> array([1, 2, 4, 0, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape 변화 없음\n",
    "x = x[idx]\n",
    "t = t[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7) (6, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_t2.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9460004620410463"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y, temp_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. first iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (3, 7) (3, 2, 7)\n"
     ]
    }
   ],
   "source": [
    "# batch size = 3\n",
    "iters = 0\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "hidden_size = 5\n",
    "batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "# 6개 target 단어에 대한 loss를 구하고 \n",
    "# 3개(3 words/ per batch)마다 W update\n",
    "# batch size = 3 -> 1 epoch = 2 iter\n",
    "# batch_t shape\n",
    "## 3: 총 6개 dataset 중 batch size=3\n",
    "## 2: window size=2에 따라 선택된 num of context words\n",
    "## 7: vocab dict size=7\n",
    "print(max_iters, batch_x.shape, batch_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st iter\n",
    "# shape: (3, 7)\n",
    "batch_t[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd iter\n",
    "batch_t[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, H = vocab_size, hidden_size\n",
    "\n",
    "# loss = model.forward(batch_x, batch_t)\n",
    "    # h = self.in_layer.forward(target)\n",
    "    # self.in_layer = MatMul(W_in)\n",
    "    # w_in = 0.01 * np.random.randn(V, H).astype('f') \n",
    "W_in = 0.01 * np.random.randn(V, H).astype('f') \n",
    "W_in_grads = [np.zeros_like(W_in)]\n",
    "h = np.dot(batch_x, W_in)   # h: dense vector\n",
    "                            # 즉, embbed word\n",
    "    # s = self.out_layer.forward(h)\n",
    "    # self.out_layer = MatMul(W_out)\n",
    "W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "W_out_grads = [np.zeros_like(W_out)]\n",
    "s = np.dot(h, W_out)     # s: output\n",
    "                        # 즉, input으로 다시 복원한 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]]\n",
      "\n",
      "[[1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]]\n",
      "\n",
      "[[0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# batch_x의 각 row = input words\n",
    "## 여기서는 3개 단어\n",
    "# temp_t1의 각 row = 각 batch_x row에 대응하는 target\n",
    "## temp_t2까지 고려하면 batch_x row에 대응하는 target은\n",
    "## window size=1 -> 2개 context 단어\n",
    "print(batch_x)\n",
    "print()\n",
    "print(temp_t1)\n",
    "print()\n",
    "print(temp_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape (3, 7)\n",
      "loss 3.8918379995748777\n"
     ]
    }
   ],
   "source": [
    "# l1 = self.loss_layer1.forward(s, contexts[:, 0])\n",
    "# self.loss_layer1 = SoftmaxWithLoss()\n",
    "# softmax(x)\n",
    "y = softmax(s)  # y는 input word가 복원되었을 때,\n",
    "                # 각 vocab에 대한 확률\n",
    "\n",
    "temp_t1 = batch_t[:, 0]\n",
    "l1 = cross_entropy_error(y, temp_t1)\n",
    "\n",
    "temp_t2 = batch_t[:, 1]\n",
    "l2 = cross_entropy_error(y, temp_t2)\n",
    "\n",
    "loss = l1 + l2\n",
    "\n",
    "print('y shape', y.shape)\n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl1 = self.loss_layer1.backward(dout)\n",
    "# self.loss_layer1 = SoftmaxWithLoss()\n",
    "#         backward\n",
    "#         batch_size = self.t.shape[0]\n",
    "batch_size = temp_t1.shape[0]\n",
    "dout = 1\n",
    "\n",
    "#         dx = self.y.copy()\n",
    "#         dx[np.arange(batch_size), self.t] -= 1\n",
    "#         dx *= dout\n",
    "#         dx = dx / batch_size\n",
    "dl1 = y.copy()\n",
    "dl1[np.arange(batch_size), temp_t1.argmax(axis=1)] -= 1 # softmax with loss: y_hat - t\n",
    "dl1 *= dout\n",
    "dl1 = dl1 / batch_size\n",
    "\n",
    "dl2= y.copy()\n",
    "dl2[np.arange(batch_size), temp_t2.argmax(axis=1)] -= 1\n",
    "dl2 *= dout\n",
    "dl2 = dl2 / batch_size\n",
    "\n",
    "# ds = dl1 + dl2\n",
    "ds = dl1 + dl2 # 분기 노드이기 때문에 합산해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 out layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dh = self.out_layer.backward(ds)\n",
    "# self.out_layer = MatMul(W_out)\n",
    "dh = np.dot(ds, W_out.T)\n",
    "dW_out = np.dot(h.T, ds)\n",
    "W_out_grads[0][...] = dW_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 in layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.in_layer.backward(dh)\n",
    "dx = np.dot(dh, W_in.T)\n",
    "dW_in = np.dot(batch_x.T, dh)\n",
    "W_in_grads[0][...] = dW_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 update W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [-4.0502448e-04,  2.9413586e-03,  2.7490219e-03, -1.4102452e-03,\n",
       "         -5.3677519e-05],\n",
       "        [ 7.3971311e-03, -2.6854158e-03, -9.5448922e-04, -1.4722566e-04,\n",
       "          1.2294482e-03],\n",
       "        [ 1.1642933e-03, -3.4094390e-03,  1.6853473e-03, -5.1500234e-03,\n",
       "         -4.2240210e-03],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00]], dtype=float32)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_in_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00031623, 0.00031623, 0.00031623, 0.00031623, 0.00031623],\n",
       "       [0.00031623, 0.00031623, 0.00031623, 0.00031623, 0.00031623],\n",
       "       [0.00031623, 0.00031623, 0.00031623, 0.00031623, 0.00031623],\n",
       "       [0.00031623, 0.00031623, 0.00031623, 0.00031623, 0.00031623],\n",
       "       [0.00031623, 0.00031623, 0.00031623, 0.00031623, 0.00031623],\n",
       "       [0.00031623, 0.00031623, 0.00031623, 0.00031623, 0.00031623],\n",
       "       [0.00031623, 0.00031623, 0.00031623, 0.00031623, 0.00031623]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sqrt(h + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam\n",
    "lr = 0.01\n",
    "h = np.zeros_like(W_in)\n",
    "W_in -= lr * W_in_grads[0] / (np.sqrt(h + 1e-7))\n",
    "\n",
    "h = np.zeros_like(W_out)\n",
    "W_out -= lr * W_out_grads[0] / (np.sqrt(h + 1e-7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
