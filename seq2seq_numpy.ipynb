{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original code: https://github.com/ma2rten/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers\n",
    "# 1.1 embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "def initialize(dim, init_range):\n",
    "    return rand(*dim) * init_range\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, init_range=1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.W = initialize((vocab_size, embed_size), init_range=1)\n",
    "        self.params = [\n",
    "            ('W', self.W, self.dW)\n",
    "        ]\n",
    "    \n",
    "    def initSequence(self):\n",
    "        self.t =0\n",
    "        self.x = {}\n",
    "        self.dW[:] = 0 # reset\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x[self.t] = x\n",
    "        self.t += 1\n",
    "        return self.W[x]\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self.t -= 1\n",
    "        x = self.x[self.t]\n",
    "        self.dW[x] += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 LSTM\n",
    "- img link: https://sergioskar.github.io/Bitcon_prediction_LSTM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://sergioskar.github.io/assets/img/posts/lstm_equations.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(*dim):\n",
    "    return np.zeros(dim)\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, init_range=1, previous=None):\n",
    "        self.input_size, self.hidden_size = input_size, hidden_size\n",
    "        \n",
    "        if previous:\n",
    "            self.previous = previous\n",
    "            previous.next = self\n",
    "        \n",
    "        def init(x, y):\n",
    "            return initialize((x, y), init_range)\n",
    "    \n",
    "        h, n = hidden_size, input_size\n",
    "        \n",
    "        self.W_hi, self.W_hf, self.W_ho, self.W_hj =\\\n",
    "            init(h, h), init(h, h), init(h, h), init(h, h)\n",
    "        self.W_xi, self.W_xf, self.W_xo, self.W_xj =\\\n",
    "            init(h, n), init(h, n), init(h, n), init(h, n)\n",
    "        self.b_i, self.b_f, self.b_o, self.b_j =\\\n",
    "            zeros(h), ones(h)*3, zeros(h), zeros(h)\n",
    "        \n",
    "        # initialize gradients\n",
    "        self.dW_hi, self.dW_hf, self.W_ho, self.W_hj =\\\n",
    "            zeros(h, h), zeros(h, h), zeros(h, h), zeros(h, h)\n",
    "        self.dW_xi, self.dW_xf, self.dW_xo, self.dW_xj =\\\n",
    "            zeros(h, n), zeros(h, n), zeros(h, n), zeros(h, n)\n",
    "        self.db_i, self.db_f, self.db_o, self.db_j =\\\n",
    "            zeros(h), zeros(h), zeros(h), zeros(h)\n",
    "        \n",
    "        # name, param, grad\n",
    "        self.params = [\n",
    "            ('W_hi', self.W_hi, self.dW_hi),\n",
    "            ('W_hf', self.W_hf, self.dW_hf),\n",
    "            ('W_ho', self.W_ho, self.dW_ho),\n",
    "            ('W_hj', self.W_hj, self.dW_hj),\n",
    "            \n",
    "            ('W_xi', self.W_xi, self.dW_xi),\n",
    "            ('W_xf', self.W_xf, self.dW_xf),\n",
    "            ('W_xo', self.W_xo, self.dW_xo),\n",
    "            ('W_xj', self.W_xj, self.dW_xj),\n",
    "            \n",
    "            ('b_i', self.b_i, self.db_i),\n",
    "            ('b_f', self.b_f, self.db_f),\n",
    "            ('b_o', self.b_o, self.db_o),\n",
    "            ('b_j', self.b_j, self.db_j),\n",
    "        ]\n",
    "        self.initSequence()\n",
    "        \n",
    "    def initSequence(self):\n",
    "        self.t = 0\n",
    "        self.x = {}\n",
    "        self.h = {}\n",
    "        self.c = {}\n",
    "        self.ct = {}\n",
    "        \n",
    "        self.input_gate = {}\n",
    "        self.forget_gate = {}\n",
    "        self.output_gate = {}\n",
    "        self.cell_update = {}\n",
    "        \n",
    "        if has_attr(self, 'previous') :\n",
    "            self.h[0] = self.previous.h[self.previous.t]\n",
    "            self.c[0] = self.previous.c[self.previous.t]\n",
    "        else:\n",
    "            self.h[0] = zeros(self.hidden_size)\n",
    "            self.c[0] = zeros(self.hidden_size)\n",
    "        \n",
    "        if has_attr(self, 'next'):\n",
    "            self.dh_prev = self.next.dh_prev\n",
    "            self.dc_prev = self.next.dc_prev\n",
    "        else:\n",
    "            self.dh_prev = zeros(self.hidden_size)\n",
    "            self.dc_prev = zeros(self.hidden_size)\n",
    "        \n",
    "        for name, param, grad in self.params:\n",
    "            grad[:] = 0\n",
    "    \n",
    "    def forward(self, x_t):\n",
    "        self.t += 1\n",
    "        \n",
    "        t = self.t\n",
    "        h = self.h[t-1]\n",
    "        \n",
    "        self.forget_gate[t] = sigmoid(np.dot(self.W_hf, h) + np.dot(self.W_xf, x_t) + self.b_f)\n",
    "        self.input_gate[t] = sigmoid(np.dot(self.W_hi, h) + np.dot(self.W_xi, x_t) + self.b_i)\n",
    "        self.output_gate[t] = sigmoid(np.dot(self.W_ho, h) + np.dot(self.W_xo, x_t) + self.b_o)\n",
    "        \n",
    "        self.cell_update[t] = tanh(np.dot(self.W_hj, h) + np.dot(self.W_xj, x_t) + self.b_j)\n",
    "            \n",
    "        self.c[t] = self.forget_gate[t] * self.c[t-1] + self.input_gate[t] * self.cell_update[t]\n",
    "        self.h[t] = self.output_gate[t] * tanh(self.c[t])\n",
    "        \n",
    "        self.x[t] = x_t\n",
    "        return self.h[t]\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        \n",
    "        t = self.t\n",
    "        \n",
    "        dh = dh + self.dh_prev\n",
    "        dC = tanh_grad(self.ct[t]) * self.output_gate[t] * dh + self.dc_prev\n",
    "        \n",
    "        # gate backprop\n",
    "        d_input = sigmoid_grad(self.input_gate[t]) * self.cell_update[t] * dC\n",
    "        d_forget = sigmoid_grad(self.forget_gate[t]) * self.c[t-1] * dC\n",
    "        d_output = sigmoid_grad(self.output_gate[t]) * self.tanh(self.c[t]) * dh\n",
    "        d_update = tanh_grad(self.cell_update[t]) * self.input_gate[t] * dC\n",
    "        \n",
    "        self.dc_prev = self.forget_gate[t] * dC\n",
    "        \n",
    "        # bias backprop\n",
    "        self.db_i += d_input\n",
    "        self.db_f += d_forget\n",
    "        self.db_o += d_output\n",
    "        self.db_j += d_update\n",
    "        \n",
    "        h_in = self.h[t-1]\n",
    "        \n",
    "        self.dW_xi += np.outer(d_input, self.x[t])\n",
    "        self.dW_xf += np.outer(d_forget, self.x[t])\n",
    "        self.dW_xo += np.outer(d_output, self.x[t])\n",
    "        self.dW_xj += np.outer(d_update, self.x[t])\n",
    "        \n",
    "        self.dW_hi += np.outer(d_input, h_in)\n",
    "        self.dW_hf += np.outer(d_forget, h_in)\n",
    "        self.dW_ho += np.outer(d_outer, h_in)\n",
    "        self.dW_hj += np.outer(d_update, h_in)\n",
    "        \n",
    "        self.dh_prev = np.dot(self.W_hi.T, d_input)\n",
    "        self.dh_prev += np.dot(self.W_hf.T, d_forget)\n",
    "        self.dh_prev += np.dot(self.W_ho.T, d_output)\n",
    "        self.dh_prev += np.dot(self.W_hj.T, d_update)\n",
    "        \n",
    "        dX = np.dot(self.W_xi.T, d_input)\n",
    "        dX += np.dot(self.W_xf.T, d_forget)\n",
    "        dX += np.dot(self.W_xo.T, d_output)\n",
    "        dX += np.dot(self.W_xj.T, d_update)\n",
    "        \n",
    "        self.t -= 1\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_size, output_size, init_range=1.0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.W = initilize((output_size, input_size), init_range)\n",
    "        self.dW = np.zeros((output_size, input_size))\n",
    "        \n",
    "        self.params = [\n",
    "            ('W', self.W, self.dW)\n",
    "        ]\n",
    "        \n",
    "    def initSequence(self):\n",
    "        self.pred = []\n",
    "        self.x = []\n",
    "        self.targets = []\n",
    "        self.t = 0\n",
    "        self.dW[:] = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.t += 1\n",
    "        \n",
    "        y = self.W.dot(x)\n",
    "        y = np.exp(y - y.max())\n",
    "        y /= y.sum()\n",
    "        \n",
    "        self.pred.append(y)\n",
    "        self.x.append(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self, target):\n",
    "        self.t -= 1\n",
    "        self.targets.append(target)\n",
    "        \n",
    "        x = self.x[self.t]\n",
    "        d = self.pred[self.t].copy()\n",
    "        d[target] -= 1\n",
    "        \n",
    "        self.dW += np.outer(d, x)\n",
    "        delta = np.dot(self.W.T, d)\n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def loss_func(self):\n",
    "        return sum(-np.log(y[target]) for target, y in zip(self.targets, reversed(self.pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "EOS = 0\n",
    "HIDDEN_SIZE = 10; EMBED_SIZE = 10; \n",
    "INPUT_SIZE = 4; OUTPUT_SIZE = 4;\n",
    "INIT_RANGE = 1.0; LEARNING_RATE = 0.7; CLIP_GRAD = 5.0\n",
    "\n",
    "class Seq2seq:\n",
    "    def __init__(self, input_size=INPUT_SIZE, outupt_size=OUTPUT_SIZE\\\n",
    "                 , hidden_size=HIDDEN_SIZE, embed_size=EMBED_SIZE\\\n",
    "                , lr=LEARNING_RATE, clip_grad=CLIP_GRAD, init_range=INIT_RANGE):\n",
    "        \n",
    "        input_layers = [\n",
    "            Embedding(output_size, embed_size, init_range),\n",
    "            LSTM(embed_size, hidden_size, init_range),\n",
    "        ]\n",
    "        \n",
    "        output_layers = [\n",
    "            Embedding(output_size, embed_size, init_range),\n",
    "            LSTM(embed_size, hidden_size, init_range, previous=input_layers[1]),\n",
    "            Softmax(hidden_size, output_size, init_range)\n",
    "        ]\n",
    "        \n",
    "        self.input_layers, self.output_layers = input_layers, output_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.clip_grad = clip_grad\n",
    "        \n",
    "    def predict(self, X, max_length=10):\n",
    "        # reset state\n",
    "        for layer in self.input_layers:\n",
    "            layer.initSequence()\n",
    "        # process input sequence\n",
    "        for x in X:\n",
    "            h = x\n",
    "            for layer in self.input_layers:\n",
    "                h = layer.forward(h)\n",
    "        \n",
    "        # reset state\n",
    "        for layer in self.output_layers:\n",
    "            layer.initSequence()\n",
    "            \n",
    "        out = []\n",
    "        token = EOS\n",
    "        \n",
    "        while len(out) < max_length:\n",
    "            h = token\n",
    "            for layer in self.output_layers:\n",
    "                h = layer.forward(h)\n",
    "                \n",
    "            token = np.argmax(h)\n",
    "            \n",
    "            if token == EOS:\n",
    "                break\n",
    "                \n",
    "            out.append(token)\n",
    "            \n",
    "    def train(self, X, Y):\n",
    "        # reset state\n",
    "        for layer in self.input_layers:\n",
    "            layer.initSequence()\n",
    "        \n",
    "        # forward pass\n",
    "        for x in X:\n",
    "            h = x\n",
    "            for layer in self.input_layers:\n",
    "                h = layer.forward(h)\n",
    "        \n",
    "        # reset state\n",
    "        for layer in self.output_layers:\n",
    "            layer.initSequence()\n",
    "        \n",
    "        for y in [EOS] + Y:\n",
    "            h = y\n",
    "            for layer in self.output_layers:\n",
    "                h = layer.forward(h)\n",
    "                \n",
    "        # backward pass\n",
    "        for y in reversed(Y + [EOS]):\n",
    "            delta = y \n",
    "            for layer in reversed(self.output_layers):\n",
    "                delta = layer.backward(delta)\n",
    "                \n",
    "        for x in reversed(X):\n",
    "            delta = np.zeros(self.hidden_size)\n",
    "            for layer in reversed(self.input_layers):\n",
    "                delta = layer.backward(delta)\n",
    "            \n",
    "        grad_norm = 0\n",
    "        \n",
    "        for layer in self.input_layers + self.output_layers:\n",
    "            for name, param, grad in layer.params:\n",
    "                grad_norm += (grad ** 2).sum()\n",
    "        \n",
    "        grad_norm = np.sqrt(grad_norm)\n",
    "        \n",
    "        for layer in self.input_layers + self.output_layers:\n",
    "            for name, param, grad in layer.params:\n",
    "                if grad_norm > self.clip_grad:\n",
    "                    grad /= grad_norm/self.clip_grad\n",
    "                param -= self.lr * grad\n",
    "                \n",
    "        return self.output_layers[-1].loss_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq -> attention -> self-attention\n",
    "- reference\n",
    "    -  https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad\n",
    "    - https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d\n",
    "- <\"Neural Machine Translation by Jointly Learning to Align and Translate\">\n",
    "- seq2seq 특징은,\n",
    "    - fixed length의 context vector: encoder의 마지막 hidden state의 결과물\n",
    "    - decoder의 h_t와 가장 관련있는 encoder의 h_n 파악\n",
    "- attention 특징은,\n",
    "    - N input word에 대하여 N개의 context vector가 존재\n",
    "    - context vector는 input sequence의 hidden state의 합계\n",
    "    - attention score은 sum of hidden state(attention score에 의해 가중곱)\n",
    "- attention 모델의 단점은, \n",
    "    - 병렬이 안됨\n",
    "    - source & target sentence의 attention반영 안됨. 즉 source & target에서 계산된 hidden로 attention 구함\n",
    "- transformer의 특징,\n",
    "    - self-attention, multi-head\n",
    "state를 사용\n",
    "- self attention으로 병렬 구현\n",
    "    - RNN & CNN 사용안함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "september 27, 1994           _1994-09-27\n",
    "August 19, 2003              _2003-08-19\n",
    "2/10/93                      _1993-02-10\n",
    "10/31/90                     _1990-10-31\n",
    "TUESDAY, SEPTEMBER 25, 1984  _1984-09-25\n",
    "JUN 17, 2013                 _2013-06-17\n",
    "april 3, 1996                _1996-04-03\n",
    "October 24, 1974             _1974-10-24\n",
    "AUGUST 11, 1986              _1986-08-11\n",
    "February 16, 2015            _2015-02-16\n",
    "October 12, 1988             _1988-10-12\n",
    "6/3/73                       _1973-06-03\n",
    "Sep 30, 1981                 _1981-09-30\n",
    "June 19, 1977                _1977-06-19\n",
    "OCTOBER 22, 2005             _2005-10-22\n",
    "December 1, 2013             _2013-12-01\n",
    "Wednesday, February 26, 1997 _1997-02-26\n",
    "TUESDAY, SEPTEMBER 10, 1991  _1991-09-10\n",
    "3/27/16                      _2016-03-27\n",
    "APRIL 14, 1983               _1983-04-14\n",
    "10/2/82                      _1982-10-02\n",
    "APR 26, 1996                 _1996-04-26\n",
    "12/21/79                     _1979-12-21\n",
    "September 4, 2012            _2012-09-04\n",
    "4/19/94                      _1994-04-19\n",
    "5/14/95                      _1995-05-14\n",
    "8/14/04                      _2004-08-14\n",
    "mar 3, 2007                  _2007-03-03\n",
    "sep 22, 2012                 _2012-09-22\n",
    "11/21/10                     _2010-11-21\n",
    "July 18, 2011                _2011-07-18\n",
    "2/22/87                      _1987-02-22\n",
    "april 15, 1987               _1987-04-15\n",
    "july 12, 2012                _2012-07-12\n",
    "june 12, 2001                _2001-06-12\n",
    "Monday, September 8, 1997    _1997-09-08\n",
    "Aug 29, 1994                 _1994-08-29\n",
    "WEDNESDAY, MARCH 20, 2002    _2002-03-20\n",
    "November 2, 1982             _1982-11-02\n",
    "Apr 22, 2004                 _2004-04-22\n",
    "April 7, 1992                _1992-04-07\n",
    "9/25/16                      _2016-09-25\n",
    "December 26, 2004            _2004-12-26\n",
    "thursday, june 5, 2008       _2008-06-05\n",
    "August 29, 2005              _2005-08-29\n",
    "March 8, 1980                _1980-03-08\n",
    "JULY 10, 2013                _2013-07-10\n",
    "9/30/04                      _2004-09-30\n",
    "APR 24, 2005                 _2005-04-24\n",
    "August 22, 1999              _1999-08-22\n",
    "5/6/93                       _1993-05-06\n",
    "OCT 22, 1975                 _1975-10-22\n",
    "10/25/97                     _1997-10-25\n",
    "6/27/76                      _1976-06-27\n",
    "5/18/13                      _2013-05-18\n",
    "November 3, 1996             _1996-11-03\n",
    "3/10/81                      _1981-03-10\n",
    "1/6/79                       _1979-01-06\n",
    "january 7, 2013              _2013-01-07\n",
    "apr 21, 2011                 _2011-04-21\n",
    "may 9, 1973                  _1973-05-09\n",
    "1/21/03                      _2003-01-21\n",
    "jun 5, 1991                  _1991-06-05\n",
    "12/27/83                     _1983-12-27\n",
    "Saturday, January 29, 2005   _2005-01-29\n",
    "JANUARY 17, 2005             _2005-01-17\n",
    "jul 22, 2014                 _2014-07-22\n",
    "6/29/11                      _2011-06-29\n",
    "Oct 13, 1996                 _1996-10-13\n",
    "Thursday, August 24, 1989    _1989-08-24\n",
    "1/28/71                      _1971-01-28\n",
    "Saturday, January 8, 2005    _2005-01-08\n",
    "Thursday, May 20, 1982       _1982-05-20\n",
    "January 3, 1991              _1991-01-03\n",
    "1/6/16                       _2016-01-06\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qes, ans = [], []\n",
    "for line in data.split('\\n')[1:-1]:\n",
    "    idx = line.find('_')\n",
    "    qes.append(line[:idx])\n",
    "    ans.append(line[idx:])\n",
    "\n",
    "id_to_char = {}\n",
    "char_to_id = {}\n",
    "for idx, _ in enumerate(qes):\n",
    "    q, a = qes[idx], ans[idx]\n",
    "    \n",
    "    chars = list(q)\n",
    "    for i, char in enumerate(chars):\n",
    "        if char not in char_to_id:\n",
    "            temp_id = len(char_to_id)\n",
    "            char_to_id[char] = temp_id\n",
    "            id_to_char[temp_id] = char\n",
    "    \n",
    "    chars = list(a)\n",
    "    for i, char in enumerate(chars):\n",
    "        if char not in char_to_id:\n",
    "            temp_id = len(char_to_id)\n",
    "            char_to_id[char] = temp_id\n",
    "            id_to_char[temp_id] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = (len(qes), len(qes[0]))\n",
    "t_shape = (len(ans), len(ans[0]))\n",
    "x = np.zeros(x_shape, dtype=np.int)\n",
    "t = np.zeros(t_shape, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(qes):\n",
    "    x[i] = [char_to_id[c] for c in list(sentence)] # [0,1,2,3,...]\n",
    "for i, sentence in enumerate(ans):\n",
    "    t[i] = [char_to_id[c] for c in list(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = np.arange(len(x))\n",
    "np.random.shuffle(indice)\n",
    "x = x[indice]\n",
    "t = t[indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_test) = x[:split_at], x[split_at:]\n",
    "(t_train, t_test) = t[:split_at], t[split_at:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52, 18, 46, ...,  7,  7,  7],\n",
       "       [11,  8, 22, ...,  7,  7,  7],\n",
       "       [11, 22, 36, ...,  7,  7,  7],\n",
       "       ...,\n",
       "       [27,  1, 41, ...,  7,  7,  7],\n",
       "       [ 0,  1,  2, ...,  7,  7,  7],\n",
       "       [17, 29, 32, ...,  7,  7,  7]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_size, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        embedded = self.dropout(self.embedding(data))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_size, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs, hidden, cell):\n",
    "        inputs = inputs.unsqueeze(0) # [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        prediction = self.fc_out(outputs.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_conda",
   "language": "python",
   "name": "torch_conda"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
