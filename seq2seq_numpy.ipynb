{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original code: https://github.com/ma2rten/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers\n",
    "# 1.1 embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "def initialize(dim, init_range):\n",
    "    return rand(*dim) * init_range\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, init_range=1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.W = initialize((vocab_size, embed_size), init_range=1)\n",
    "        self.params = [\n",
    "            ('W', self.W, self.dW)\n",
    "        ]\n",
    "    \n",
    "    def initSequence(self):\n",
    "        self.t =0\n",
    "        self.x = {}\n",
    "        self.dW[:] = 0 # reset\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x[self.t] = x\n",
    "        self.t += 1\n",
    "        return self.W[x]\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self.t -= 1\n",
    "        x = self.x[self.t]\n",
    "        self.dW[x] += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 LSTM\n",
    "- img link: https://sergioskar.github.io/Bitcon_prediction_LSTM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://sergioskar.github.io/assets/img/posts/lstm_equations.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(*dim):\n",
    "    return np.zeros(dim)\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, init_range=1, previous=None):\n",
    "        self.input_size, self.hidden_size = input_size, hidden_size\n",
    "        \n",
    "        if previous:\n",
    "            self.previous = previous\n",
    "            previous.next = self\n",
    "        \n",
    "        def init(x, y):\n",
    "            return initialize((x, y), init_range)\n",
    "    \n",
    "        h, n = hidden_size, input_size\n",
    "        \n",
    "        self.W_hi, self.W_hf, self.W_ho, self.W_hj =\\\n",
    "            init(h, h), init(h, h), init(h, h), init(h, h)\n",
    "        self.W_xi, self.W_xf, self.W_xo, self.W_xj =\\\n",
    "            init(h, n), init(h, n), init(h, n), init(h, n)\n",
    "        self.b_i, self.b_f, self.b_o, self.b_j =\\\n",
    "            zeros(h), ones(h)*3, zeros(h), zeros(h)\n",
    "        \n",
    "        # initialize gradients\n",
    "        self.dW_hi, self.dW_hf, self.W_ho, self.W_hj =\\\n",
    "            zeros(h, h), zeros(h, h), zeros(h, h), zeros(h, h)\n",
    "        self.dW_xi, self.dW_xf, self.dW_xo, self.dW_xj =\\\n",
    "            zeros(h, n), zeros(h, n), zeros(h, n), zeros(h, n)\n",
    "        self.db_i, self.db_f, self.db_o, self.db_j =\\\n",
    "            zeros(h), zeros(h), zeros(h), zeros(h)\n",
    "        \n",
    "        # name, param, grad\n",
    "        self.params = [\n",
    "            ('W_hi', self.W_hi, self.dW_hi),\n",
    "            ('W_hf', self.W_hf, self.dW_hf),\n",
    "            ('W_ho', self.W_ho, self.dW_ho),\n",
    "            ('W_hj', self.W_hj, self.dW_hj),\n",
    "            \n",
    "            ('W_xi', self.W_xi, self.dW_xi),\n",
    "            ('W_xf', self.W_xf, self.dW_xf),\n",
    "            ('W_xo', self.W_xo, self.dW_xo),\n",
    "            ('W_xj', self.W_xj, self.dW_xj),\n",
    "            \n",
    "            ('b_i', self.b_i, self.db_i),\n",
    "            ('b_f', self.b_f, self.db_f),\n",
    "            ('b_o', self.b_o, self.db_o),\n",
    "            ('b_j', self.b_j, self.db_j),\n",
    "        ]\n",
    "        self.initSequence()\n",
    "        \n",
    "    def initSequence(self):\n",
    "        self.t = 0\n",
    "        self.x = {}\n",
    "        self.h = {}\n",
    "        self.c = {}\n",
    "        self.ct = {}\n",
    "        \n",
    "        self.input_gate = {}\n",
    "        self.forget_gate = {}\n",
    "        self.output_gate = {}\n",
    "        self.cell_update = {}\n",
    "        \n",
    "        if has_attr(self, 'previous') :\n",
    "            self.h[0] = self.previous.h[self.previous.t]\n",
    "            self.c[0] = self.previous.c[self.previous.t]\n",
    "        else:\n",
    "            self.h[0] = zeros(self.hidden_size)\n",
    "            self.c[0] = zeros(self.hidden_size)\n",
    "        \n",
    "        if has_attr(self, 'next'):\n",
    "            self.dh_prev = self.next.dh_prev\n",
    "            self.dc_prev = self.next.dc_prev\n",
    "        else:\n",
    "            self.dh_prev = zeros(self.hidden_size)\n",
    "            self.dc_prev = zeros(self.hidden_size)\n",
    "        \n",
    "        for name, param, grad in self.params:\n",
    "            grad[:] = 0\n",
    "    \n",
    "    def forward(self, x_t):\n",
    "        self.t += 1\n",
    "        \n",
    "        t = self.t\n",
    "        h = self.h[t-1]\n",
    "        \n",
    "        self.forget_gate[t] = sigmoid(np.dot(self.W_hf, h) + np.dot(self.W_xf, x_t) + self.b_f)\n",
    "        self.input_gate[t] = sigmoid(np.dot(self.W_hi, h) + np.dot(self.W_xi, x_t) + self.b_i)\n",
    "        self.output_gate[t] = sigmoid(np.dot(self.W_ho, h) + np.dot(self.W_xo, x_t) + self.b_o)\n",
    "        \n",
    "        self.cell_update[t] = tanh(np.dot(self.W_hj, h) + np.dot(self.W_xj, x_t) + self.b_j)\n",
    "            \n",
    "        self.c[t] = self.forget_gate[t] * self.c[t-1] + self.input_gate[t] * self.cell_update[t]\n",
    "        self.h[t] = self.output_gate[t] * tanh(self.c[t])\n",
    "        \n",
    "        self.x[t] = x_t\n",
    "        return self.h[t]\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        \n",
    "        t = self.t\n",
    "        \n",
    "        dh = dh + self.dh_prev\n",
    "        dC = tanh_grad(self.ct[t]) * self.output_gate[t] * dh + self.dc_prev\n",
    "        \n",
    "        # gate backprop\n",
    "        d_input = sigmoid_grad(self.input_gate[t]) * self.cell_update[t] * dC\n",
    "        d_forget = sigmoid_grad(self.forget_gate[t]) * self.c[t-1] * dC\n",
    "        d_output = sigmoid_grad(self.output_gate[t]) * self.tanh(self.c[t]) * dh\n",
    "        d_update = tanh_grad(self.cell_update[t]) * self.input_gate[t] * dC\n",
    "        \n",
    "        self.dc_prev = self.forget_gate[t] * dC\n",
    "        \n",
    "        # bias backprop\n",
    "        self.db_i += d_input\n",
    "        self.db_f += d_forget\n",
    "        self.db_o += d_output\n",
    "        self.db_j += d_update\n",
    "        \n",
    "        h_in = self.h[t-1]\n",
    "        \n",
    "        self.dW_xi += np.outer(d_input, self.x[t])\n",
    "        self.dW_xf += np.outer(d_forget, self.x[t])\n",
    "        self.dW_xo += np.outer(d_output, self.x[t])\n",
    "        self.dW_xj += np.outer(d_update, self.x[t])\n",
    "        \n",
    "        self.dW_hi += np.outer(d_input, h_in)\n",
    "        self.dW_hf += np.outer(d_forget, h_in)\n",
    "        self.dW_ho += np.outer(d_outer, h_in)\n",
    "        self.dW_hj += np.outer(d_update, h_in)\n",
    "        \n",
    "        self.dh_prev = np.dot(self.W_hi.T, d_input)\n",
    "        self.dh_prev += np.dot(self.W_hf.T, d_forget)\n",
    "        self.dh_prev += np.dot(self.W_ho.T, d_output)\n",
    "        self.dh_prev += np.dot(self.W_hj.T, d_update)\n",
    "        \n",
    "        dX = np.dot(self.W_xi.T, d_input)\n",
    "        dX += np.dot(self.W_xf.T, d_forget)\n",
    "        dX += np.dot(self.W_xo.T, d_output)\n",
    "        dX += np.dot(self.W_xj.T, d_update)\n",
    "        \n",
    "        self.t -= 1\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_size, output_size, init_range=1.0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.W = initilize((output_size, input_size), init_range)\n",
    "        self.dW = np.zeros((output_size, input_size))\n",
    "        \n",
    "        self.params = [\n",
    "            ('W', self.W, self.dW)\n",
    "        ]\n",
    "        \n",
    "    def initSequence(self):\n",
    "        self.pred = []\n",
    "        self.x = []\n",
    "        self.targets = []\n",
    "        self.t = 0\n",
    "        self.dW[:] = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.t += 1\n",
    "        \n",
    "        y = self.W.dot(x)\n",
    "        y = np.exp(y - y.max())\n",
    "        y /= y.sum()\n",
    "        \n",
    "        self.pred.append(y)\n",
    "        self.x.append(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self, target):\n",
    "        self.t -= 1\n",
    "        self.targets.append(target)\n",
    "        \n",
    "        x = self.x[self.t]\n",
    "        d = self.pred[self.t].copy()\n",
    "        d[target] -= 1\n",
    "        \n",
    "        self.dW += np.outer(d, x)\n",
    "        delta = np.dot(self.W.T, d)\n",
    "        \n",
    "        return delta\n",
    "    \n",
    "    def loss_func(self):\n",
    "        return sum(-np.log(y[target]) for target, y in zip(self.targets, reversed(self.pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "\n",
    "EOS = 0\n",
    "HIDDEN_SIZE = 10; EMBED_SIZE = 10; \n",
    "INPUT_SIZE = 4; OUTPUT_SIZE = 4;\n",
    "INIT_RANGE = 1.0; LEARNING_RATE = 0.7; CLIP_GRAD = 5.0\n",
    "\n",
    "class Seq2seq:\n",
    "    def __init__(self, input_size=INPUT_SIZE, outupt_size=OUTPUT_SIZE\\\n",
    "                 , hidden_size=HIDDEN_SIZE, embed_size=EMBED_SIZE\\\n",
    "                , lr=LEARNING_RATE, clip_grad=CLIP_GRAD, init_range=INIT_RANGE):\n",
    "        \n",
    "        input_layers = [\n",
    "            Embedding(output_size, embed_size, init_range),\n",
    "            LSTM(embed_size, hidden_size, init_range),\n",
    "        ]\n",
    "        \n",
    "        output_layers = [\n",
    "            Embedding(output_size, embed_size, init_range),\n",
    "            LSTM(embed_size, hidden_size, init_range, previous=input_layers[1]),\n",
    "            Softmax(hidden_size, output_size, init_range)\n",
    "        ]\n",
    "        \n",
    "        self.input_layers, self.output_layers = input_layers, output_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.clip_grad = clip_grad\n",
    "        \n",
    "    def predict(self, X, max_length=10):\n",
    "        # reset state\n",
    "        for layer in self.input_layers:\n",
    "            layer.initSequence()\n",
    "        # process input sequence\n",
    "        for x in X:\n",
    "            h = x\n",
    "            for layer in self.input_layers:\n",
    "                h = layer.forward(h)\n",
    "        \n",
    "        # reset state\n",
    "        for layer in self.output_layers:\n",
    "            layer.initSequence()\n",
    "            \n",
    "        out = []\n",
    "        token = EOS\n",
    "        \n",
    "        while len(out) < max_length:\n",
    "            h = token\n",
    "            for layer in self.output_layers:\n",
    "                h = layer.forward(h)\n",
    "                \n",
    "            token = np.argmax(h)\n",
    "            \n",
    "            if token == EOS:\n",
    "                break\n",
    "                \n",
    "            out.append(token)\n",
    "            \n",
    "    def train(self, X, Y):\n",
    "        # reset state\n",
    "        for layer in self.input_layers:\n",
    "            layer.initSequence()\n",
    "        \n",
    "        # forward pass\n",
    "        for x in X:\n",
    "            h = x\n",
    "            for layer in self.input_layers:\n",
    "                h = layer.forward(h)\n",
    "        \n",
    "        # reset state\n",
    "        for layer in self.output_layers:\n",
    "            layer.initSequence()\n",
    "        \n",
    "        for y in [EOS] + Y:\n",
    "            h = y\n",
    "            for layer in self.output_layers:\n",
    "                h = layer.forward(h)\n",
    "                \n",
    "        # backward pass\n",
    "        for y in reversed(Y + [EOS]):\n",
    "            delta = y \n",
    "            for layer in reversed(self.output_layers):\n",
    "                delta = layer.backward(delta)\n",
    "                \n",
    "        for x in reversed(X):\n",
    "            delta = np.zeros(self.hidden_size)\n",
    "            for layer in reversed(self.input_layers):\n",
    "                delta = layer.backward(delta)\n",
    "            \n",
    "        grad_norm = 0\n",
    "        \n",
    "        for layer in self.input_layers + self.output_layers:\n",
    "            for name, param, grad in layer.params:\n",
    "                grad_norm += (grad ** 2).sum()\n",
    "        \n",
    "        grad_norm = np.sqrt(grad_norm)\n",
    "        \n",
    "        for layer in self.input_layers + self.output_layers:\n",
    "            for name, param, grad in layer.params:\n",
    "                if grad_norm > self.clip_grad:\n",
    "                    grad /= grad_norm/self.clip_grad\n",
    "                param -= self.lr * grad\n",
    "                \n",
    "        return self.output_layers[-1].loss_func()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq -> attention -> self-attention\n",
    "- reference\n",
    "    -  https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad\n",
    "    - https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d\n",
    "- <\"Neural Machine Translation by Jointly Learning to Align and Translate\">\n",
    "- seq2seq 특징은,\n",
    "    - fixed length의 context vector: encoder의 마지막 hidden state의 결과물\n",
    "    - decoder의 h_t와 가장 관련있는 encoder의 h_n 파악\n",
    "- attention 특징은,\n",
    "    - N input word에 대하여 N개의 context vector가 존재\n",
    "    - context vector는 input sequence의 hidden state의 합계\n",
    "    - attention score은 sum of hidden state(attention score에 의해 가중곱)\n",
    "- attention 모델의 단점은, \n",
    "    - 병렬이 안됨\n",
    "    - source & target sentence의 attention반영 안됨. 즉 source & target에서 계산된 hidden로 attention 구함\n",
    "- transformer의 특징,\n",
    "    - self-attention, multi-head\n",
    "state를 사용\n",
    "- self attention으로 병렬 구현\n",
    "    - RNN & CNN 사용안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
